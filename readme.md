# Official PyTorch implementation of "Tackling the Generative Learning Trilemma with Denoising Diffusion GANs" [(ICLR 2022 Spotlight Paper)](https://arxiv.org/abs/2112.07804) #

<div align="center">
  <a href="https://xavierxiao.github.io/" target="_blank">Zhisheng&nbsp;Xiao</a> &emsp; <b>&middot;</b> &emsp;
  <a href="https://karstenkreis.github.io/" target="_blank">Karsten&nbsp;Kreis</a> &emsp; <b>&middot;</b> &emsp;
  <a href="http://latentspace.cc/arash_vahdat/" target="_blank">Arash&nbsp;Vahdat</a>
  <br> <br>
  <a href="https://nvlabs.github.io/denoising-diffusion-gan/" target="_blank">Project&nbsp;Page</a>
</div>
<br>
<br>

<div align="center">
    <img width="800" alt="teaser" src="assets/teaser.png"/>
</div>

Generative denoising diffusion models typically assume that the denoising distribution can be modeled by a Gaussian distribution. This assumption holds only for small denoising steps, which in practice translates to thousands of denoising steps in the synthesis process. In our denoising diffusion GANs, we represent the denoising model using multimodal and complex conditional GANs, enabling us to efficiently generate data in as few as two steps.

## Set up datasets ##
We trained on several datasets, including CIFAR10, LSUN Church Outdoor 256 and CelebA HQ 256. 
For large datasets, we store the data in LMDB datasets for I/O efficiency. Check [here](https://github.com/NVlabs/NVAE#set-up-file-paths-and-data) for information regarding dataset preparation.


## Training Denoising Diffusion GANs ##
We use the following commands on each dataset for training denoising diffusion GANs.

#### CIFAR-10 ####

We train Denoising Diffusion GANs on CIFAR-10 using 4 32-GB V100 GPU. 
```
python3 train_ddgan.py --dataset cifar10 --data_root  --exp ddgan_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 \
--num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional \
--use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 4 \
--ch_mult 1 2 2 2 --save_content

CUDA_LAUNCH_BLOCKING=1 python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 \
--num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional \
--use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 \
--ch_mult 1 2 2 2 --save_content --master_port 6220 --cfg_T 5 --cfg_eta 1


```

#### LSUN Church Outdoor 256 ####

We train Denoising Diffusion GANs on LSUN Church Outdoor 256 using 8 32-GB V100 GPU. 
```
python3 train_ddgan.py --dataset lsun --image_size 256 --exp ddgan_lsun_exp1 --num_channels 3 --num_channels_dae 64 --ch_mult 1 1 2 2 4 4 --num_timesteps 4 \
--num_res_blocks 2 --batch_size 8 --num_epoch 500 --ngf 64 --embedding_type positional --use_ema --ema_decay 0.999 --r1_gamma 1. \
--z_emb_dim 256 --lr_d 1e-4 --lr_g 1.6e-4 --lazy_reg 10 --num_process_per_node 8 --save_content
```

#### CelebA HQ 256 ####

We train Denoising Diffusion GANs on CelebA HQ 256 using 8 32-GB V100 GPUs. 
```
python3 train_ddgan.py --dataset celeba_256 --image_size 256 --exp ddgan_celebahq_exp1 --num_channels 3 --num_channels_dae 64 --ch_mult 1 1 2 2 4 4 --num_timesteps 2 \
--num_res_blocks 2 --batch_size 4 --num_epoch 800 --ngf 64 --embedding_type positional --use_ema --r1_gamma 2. \
--z_emb_dim 256 --lr_d 1e-4 --lr_g 2e-4 --lazy_reg 10  --num_process_per_node 8 --save_content
```

## Pretrained Checkpoints ##
We have released pretrained checkpoints on CIFAR-10 and CelebA HQ 256 at this 
[Google drive directory](https://drive.google.com/drive/folders/1UkzsI0SwBRstMYysRdR76C1XdSv5rQNz?usp=sharing).
Simply download the `saved_info` directory to the code directory. Use `--epoch_id 1200` for CIFAR-10 and `--epoch_id 550`
for CelebA HQ 256 in the commands below.

## Evaluation ##
After training, samples can be generated by calling ```test_ddgan.py```. We evaluate the models with single V100 GPU.
Below, we use `--epoch_id` to specify the checkpoint saved at a particular epoch.
Specifically, for models trained by above commands, the scripts for generating samples on CIFAR-10 is
```
python3 test_ddgan.py --dataset cifar10 --exp ddgan_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 \
--num_res_blocks 2 --nz 100 --z_emb_dim 256 --n_mlp 4 --ch_mult 1 2 2 2 --epoch_id $EPOCH
```
The scripts for generating samples on CelebA HQ is 
```
python3 test_ddgan.py --dataset celeba_256 --image_size 256 --exp ddgan_celebahq_exp1 --num_channels 3 --num_channels_dae 64 \
--ch_mult 1 1 2 2 4 4 --num_timesteps 2 --num_res_blocks 2  --epoch_id $EPOCH
```
The scripts for generating samples on LSUN Church Outdoor is 
```
python3 test_ddgan.py --dataset lsun --image_size 256 --exp ddgan_lsun_exp1 --num_channels 3 --num_channels_dae 64 \
--ch_mult 1 1 2 2 4 4  --num_timesteps 4 --num_res_blocks 2  --epoch_id $EPOCH
```

We use the [PyTorch](https://github.com/mseitzer/pytorch-fid) implementation to compute the FID scores, and in particular, codes for computing the FID are adapted from [FastDPM](https://github.com/FengNiMa/FastDPM_pytorch).

To compute FID, run the same scripts above for sampling, with additional arguments ```--compute_fid``` and ```--real_img_dir /path/to/real/images```.

For Inception Score, save samples in a single numpy array with pixel values in range [0, 255] and simply run 
```
python ./pytorch_fid/inception_score.py --sample_dir /path/to/sampled_images
```
where the code for computing Inception Score is adapted from [here](https://github.com/tsc2017/Inception-Score).

For Improved Precision and Recall, follow the instruction [here](https://github.com/kynkaat/improved-precision-and-recall-metric).


## License ##
Please check the LICENSE file. Denoising diffusion GAN may be used non-commercially, meaning for research or 
evaluation purposes only. For business inquiries, please contact 
[researchinquiries@nvidia.com](mailto:researchinquiries@nvidia.com).

## Bibtex ##
Cite our paper using the following bibtex item:

```
@inproceedings{
xiao2022tackling,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion GANs},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2022}
}
```

## Contributors ##
Denoising Diffusion GAN was built primarily by [Zhisheng Xiao](https://xavierxiao.github.io/) during a summer 
internship at NVIDIA research.

python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6222 --cfg_T 15 --cfg_eta 5 --gen dgan_cfg_cifar10_exp1_cfgt15_eta5_tim4_v1/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta5_cfgt15_time4_v1

python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 8 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6223 --cfg_T 15 --cfg_eta 5 --gen dgan_cfg_cifar10_exp1_cfgt10_time8/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta1_cfgt10_time8

python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 4e-5 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6221 --cfg_T 15 --cfg_eta 1 --gen dgan_cfg_cifar10_exp1_cfgt15_tim4/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta1_cfgt15_time4


python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6223 --cfg_T 15 --cfg_eta 5 --gen dgan_cfg_cifar10_exp1_cfgt15_time4_eta5_good/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta5_cfgt15_time4_good --num_gen 64


python3 train_ddgan.py --dataset stackmnist --data_root ../ --exp ddgan_cfg_stackmnist_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6221 --cfg_T 15 --cfg_eta 1 --gen dgan_cfg_stackmnist_exp1_cfgt15_tim4_eta1_new/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan-cfg-eta1-cfgt15-time4-stackmnist-new --num_gen 64

python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6226 --cfg_T 15 --cfg_eta 0.5 --gen dgan_cfg_cifar10_exp1_cfgt15_time4_eta0.5_good/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta0.5_cfgt15_time4_good --num_gen 64

python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6227 --cfg_T 15 --cfg_eta 10 --gen dgan_cfg_cifar10_exp1_cfgt15_time4_eta10_good/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta10_cfgt15_time4_good --num_gen 64



python3 train_ddgan.py --dataset lsun64 --data_root /raid/datasets/LSUN/LSUN --exp ddgan_cfg_lsun_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 1 2 2 4 --save_content --master_port 6227 --cfg_T 15 --cfg_eta 1 --gen dgan_cfg_church_exp1_cfgt15_time4_eta1_good/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta1_church_cfgt15_time4_good --num_gen 64 --image_size 64


python3 train_ddgan.py --dataset lsun_bedroom64 --data_root /raid/datasets/LSUN/ --exp ddgan_cfg_lsun_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 4 --save_content --master_port 6228 --cfg_T 15 --cfg_eta 1 --gen dgan_cfg_bedroom_exp1_cfgt15_time4_eta1_good/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta1_bedroom_cfgt15_time4_good --num_gen 64


python3 train_ddgan.py --dataset lsun64 --data_root /raid/datasets/LSUN/LSUN --exp ddgan_cfg_lsun_exp1 --num_channels 3 --num_channels_dae 64 --num_timesteps 4 --num_res_blocks 2 --batch_size 8 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.6e-5 --lr_g 1.6e-5 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 1 2 2 4 4 --save_content --master_port 6223 --cfg_T 15 --cfg_eta 1 --gen dgan_cfg_church256_exp1_cfgt15_time4_eta1_good/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta1_church256_cfgt15_time4_good --num_gen 8 --image_size 256

python3 train_ddgan.py --dataset Imagenet64 --data_root /raid/datasets/Imagenet/imagenet_train --exp ddgan_cfg_lsun_exp1 --num_channels 3 --num_channels_dae 64 --num_timesteps 4 --num_res_blocks 2 --batch_size 32 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.6e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 1 2 2 4 4 --save_content --master_port 6230 --cfg_T 15 --cfg_eta 1 --gen dgan_cfg_imagenet64_exp1_cfgt15_time4_eta1_good/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta1_imagent64_cfgt15_time4_good --num_gen 32 --image_size 64


python3 train_ddgan.py --dataset lsun64 --data_root /raid/datasets/LSUN/LSUN --exp ddgan_cfg_lsun_exp1 --num_channels 3 --num_channels_dae 64 --num_timesteps 4 --num_res_blocks 2 --batch_size 8 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 8e-5 --lr_g 8e-5 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 1 2 2 4 4 --save_content --master_port 6220 --cfg_T 15 --cfg_eta 10 --gen dgan_cfg_church256_exp1_cfgt15_time4_eta1_good/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta1_church256_cfgt15_time4_good --num_gen 8 --image_size 256

/raid/datasets/Imagenet/imagenet_train

------test
python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 2 --num_res_blocks 2 --batch_size 64 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 1.25e-4 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6223 --cfg_T 15 --cfg_eta 1  --gen_interval 2 --save_interval 2

python3 train_ddgan.py --dataset cifar10 --data_root ../ --exp ddgan_cfg_cifar10_exp1 --num_channels 3 --num_channels_dae 128 --num_timesteps 4 --num_res_blocks 2 --batch_size 32 --num_epoch 1800 --ngf 64 --nz 100 --z_emb_dim 256 --n_mlp 4 --embedding_type positional --use_ema --ema_decay 0.9999 --r1_gamma 0.02 --lr_d 1.25e-4 --lr_g 4e-5 --lazy_reg 15 --num_process_per_node 1 --ch_mult 1 2 2 2 --save_content --master_port 6221 --cfg_T 15 --cfg_eta 1 --gen dgan_cfg_cifar10_exp1_cfgt15_tim4/dgan_cfg --gen_interval 50 --save_interval 1000 --save mod/ddgan_cfg_eta1_cfgt15_time4



export CUDA_VISIBLE_DEVICES="2"
----normal-----
tensor([ 1.1008e-07, -1.5587e-05,  3.9859e-06,  5.7265e-06,  7.4914e-06,
        -2.0589e-06, -4.1199e-05, -1.1958e-05, -9.3894e-07, -2.2966e-06,
        -2.2911e-05, -2.0799e-06, -1.4541e-05, -9.9937e-06,  4.1552e-08,
        -3.6208e-05,  1.9089e-06, -8.7335e-06, -6.0816e-06,  4.6805e-06,
        -2.5859e-05, -7.0085e-06,  4.1569e-06, -2.0402e-06, -2.2101e-05,
        -6.4222e-06,  6.3859e-06, -1.6546e-05, -2.8470e-06, -1.1143e-06,
         2.3905e-06,  6.0158e-06,  1.7569e-06,  5.4983e-07, -2.9968e-06,
        -4.9847e-06, -2.0461e-06,  1.3410e-05, -1.0427e-05, -1.6986e-05,
        -2.8245e-06,  5.7943e-06, -2.8410e-05,  1.2743e-07,  1.3985e-06,
        -3.6288e-06,  8.1920e-07, -2.0296e-07, -1.2622e-05, -3.3728e-05,
         3.9796e-06, -1.7959e-05, -4.4960e-05, -6.0305e-06, -4.8064e-07,
        -1.7435e-05, -3.9544e-06, -4.9057e-05, -2.3690e-05, -1.5891e-05,
         1.5526e-06, -3.1401e-06, -5.9242e-06, -2.6385e-06], device='cuda:0',
       grad_fn=<ViewBackward0>)
tensor([-3.8685e-06, -1.4226e-05,  1.7759e-06, -7.0173e-07,  4.2013e-06,
        -4.4503e-06, -2.1456e-05, -5.7684e-06,  5.5757e-07,  1.6990e-06,
        -9.0553e-06, -4.7729e-06, -1.1051e-05, -5.1847e-06,  1.9705e-06,
        -1.9786e-05,  3.3765e-06, -1.1289e-05, -5.5852e-06,  4.5242e-06,
        -1.1456e-05, -7.5534e-06, -2.0083e-06,  1.5403e-06, -1.2676e-05,
        -2.7249e-07,  4.0999e-06, -5.9621e-06, -3.0600e-06, -2.6013e-08,
         3.8166e-06,  3.4914e-06, -1.0506e-06,  1.0816e-06, -1.9415e-06,
        -1.4312e-06,  1.3400e-06,  9.3252e-06, -1.3256e-05, -3.7005e-06,
         8.2649e-07, -1.3996e-08, -1.4038e-05, -3.8063e-06,  2.7024e-06,
        -2.4211e-06,  2.5511e-06, -3.0596e-06, -6.7923e-06, -2.0786e-05,
        -2.9789e-07, -9.2235e-06, -2.2719e-05, -5.4327e-06,  1.1658e-06,
        -1.4289e-05,  9.2416e-07, -1.9573e-05, -1.5249e-05, -9.2936e-06,
        -3.5992e-06, -3.3832e-06, -4.1411e-06, -1.6866e-06], device='cuda:0',

-------error----
tensor([-446.9474, -425.7914, -418.9248, -426.3686, -420.1763, -439.1586,
        -421.6883, -423.9076, -426.8852, -414.6456, -419.7941, -432.3446,
        -438.6101, -408.8016, -439.1223, -413.6248, -433.0167, -449.0703,
        -419.7922, -437.5508, -435.2507, -444.6638, -424.7978, -432.3343,
        -425.9358, -413.3918, -440.9738, -432.1096, -429.0591, -414.7708,
        -419.2281, -455.5177, -432.7215, -423.3076, -426.7935, -413.8935,
        -423.4511, -429.7169, -459.5619, -425.1099, -476.8150, -433.9433,
        -489.9538, -433.0069, -429.1754, -413.7296, -419.7934, -411.5046,
        -436.9219, -421.6159, -468.1855, -426.5120, -441.6016, -423.0869,
        -420.4574, -437.2161, -424.6809, -444.2659, -440.4728, -438.1578,
        -443.0558, -412.2796, -421.1496, -427.5021], device='cuda:0',
       grad_fn=<ViewBackward0>)
tensor([-364.9498, -367.3510, -368.4781, -364.9907, -369.9515, -364.3181,
        -366.3249, -366.6125, -368.9102, -369.9161, -371.3416, -366.1664,
        -363.9014, -369.2087, -361.1546, -366.0040, -367.9012, -364.2716,
        -369.0197, -364.0313, -363.8012, -363.1258, -366.5622, -363.0913,
        -368.5625, -370.1893, -364.4562, -367.1500, -366.0860, -370.1139,
        -366.8993, -363.0747, -364.2552, -368.0177, -369.2014, -367.9053,
        -368.5886, -365.9823, -358.6046, -368.9389, -357.8473, -365.3996,
        -355.3480, -364.3689, -366.9823, -367.3305, -370.5460, -368.9711,
        -365.7666, -368.3624, -356.7359, -366.1859, -361.3357, -367.8768,
        -368.7869, -364.8984, -366.4869, -362.0519, -363.2751, -365.0865,
        -366.2812, -367.9447, -368.8814, -367.0464], device='cuda:0',
       grad_fn=<ViewBackward0>)
